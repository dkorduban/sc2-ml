{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 6, 30, 30)\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "a = np.load('/Users/dkorduban/workspace/sc2/my/replays/npz/a.npz')\n",
    "batch = a['both']\n",
    "print(batch.shape)\n",
    "print(len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.Tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 6, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ortho_weights(shape, scale=1.):\n",
    "    \"\"\" PyTorch port of ortho_init from baselines.a2c.utils \"\"\"\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        flat_shape = shape[1], shape[0]\n",
    "    elif len(shape) == 4:\n",
    "        flat_shape = (np.prod(shape[1:]), shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    a = np.random.normal(0., 1., flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.transpose().copy().reshape(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        return torch.from_numpy((scale * q).astype(np.float32))\n",
    "    if len(shape) == 4:\n",
    "        return torch.from_numpy((scale * q[:, :shape[1], :shape[2]]).astype(np.float32))\n",
    "\n",
    "\n",
    "def atari_initializer(module):\n",
    "    \"\"\" Parameter initializer for Atari models\n",
    "\n",
    "    Initializes Linear, Conv2d, and LSTM weights.\n",
    "    \"\"\"\n",
    "    classname = module.__class__.__name__\n",
    "\n",
    "    if classname == 'Linear':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'Conv2d':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'LSTM':\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'weight_hh' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'bias' in name:\n",
    "                param.data.zero_()\n",
    "\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        \"\"\" Basic convolutional variational autoencoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, 4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        last_dim = 128 * 4\n",
    "\n",
    "        self.mu = nn.Linear(last_dim, z_dim)\n",
    "        self.logvar = nn.Linear(last_dim, z_dim)\n",
    "        \n",
    "        self.\n",
    "        \n",
    "    def forward(self, conv_in):\n",
    "        \"\"\" Module forward pass\n",
    "\n",
    "        Args:\n",
    "            conv_in (Variable): convolutional input, shaped [N x 4 x 84 x 84]\n",
    "\n",
    "        Returns:\n",
    "            pi (Variable): action probability logits, shaped [N x self.num_actions]\n",
    "            v (Variable): value predictions, shaped [N x 1]\n",
    "        \"\"\"\n",
    "        N = conv_in.size()[0]\n",
    "\n",
    "        conv_out = self.conv(conv_in).view(N, 64 * 7 * 7)\n",
    "\n",
    "        fc_out = self.fc(conv_out)\n",
    "\n",
    "        pi_out = self.pi(fc_out)\n",
    "        v_out = self.v(fc_out)\n",
    "\n",
    "        return pi_out, v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.modules.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.RNNBase??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        a = np.load(path)\n",
    "        self._data = a['both']\n",
    "    def __getitem__(self, index):\n",
    "        return self._data[index]\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "dataset = MySimpleDataset('/Users/dkorduban/workspace/sc2/my/replays/npz/a.npz')\n",
    "loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10, 64, 2, 2])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 32]) torch.Size([10, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lagom.core.networks import make_fc\n",
    "from lagom.core.networks import make_cnn\n",
    "from lagom.core.networks import make_transposed_cnn\n",
    "\n",
    "from lagom.core.networks import ortho_init\n",
    "\n",
    "from lagom.core.networks import BaseVAE\n",
    "\n",
    "class ConvVAE(BaseVAE):\n",
    "    def make_encoder(self, config):\n",
    "        out = make_cnn(input_channel=1, \n",
    "                       channels=[64, 64, 64], \n",
    "                       kernels=[4, 4, 4], \n",
    "                       strides=[2, 2, 1], \n",
    "                       paddings=[0, 0, 0])\n",
    "        last_dim = 256\n",
    "        \n",
    "        return out, last_dim\n",
    "        \n",
    "    def make_moment_heads(self, config, last_dim):\n",
    "        out = {}\n",
    "        \n",
    "        z_dim = config['network.z_dim']\n",
    "        \n",
    "        out['mu_head'] = nn.Linear(in_features=last_dim, out_features=z_dim)\n",
    "        out['logvar_head'] = nn.Linear(in_features=last_dim, out_features=z_dim)\n",
    "        out['z_dim'] = z_dim\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def make_decoder(self, config, z_dim):\n",
    "        out = nn.ModuleList()\n",
    "        \n",
    "        out.append(nn.Linear(in_features=z_dim, out_features=self.last_dim))\n",
    "        \n",
    "        out.extend(make_transposed_cnn(input_channel=64, \n",
    "                                       channels=[64, 64, 64], \n",
    "                                       kernels=[4, 4, 4], \n",
    "                                       strides=[2, 1, 1], \n",
    "                                       paddings=[0, 0, 0], \n",
    "                                       output_paddings=[0, 0, 0]))\n",
    "        \n",
    "        out.append(nn.Linear(in_features=9216, out_features=28*28*1))\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def init_params(self, config):\n",
    "        for layer in self.encoder:\n",
    "            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n",
    "            \n",
    "        ortho_init(self.mu_head, nonlinearity=None, weight_scale=0.01, constant_bias=0.0)\n",
    "        ortho_init(self.logvar_head, nonlinearity=None, weight_scale=0.01, constant_bias=0.0)\n",
    "        \n",
    "        for layer in self.decoder:\n",
    "            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n",
    "\n",
    "    def encoder_forward(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder_forward(self, z):\n",
    "        # Forward of first fully-connected layer\n",
    "        x = F.relu(self.decoder[0](z))\n",
    "        \n",
    "        # Reshape as [NxCxHxW]\n",
    "        x = x.view(-1, 64, 2, 2)\n",
    "        \n",
    "        # Forward pass through transposed convolutional layer\n",
    "        for layer in self.decoder[1:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "        # Flatten to [N, D]\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Element-wise binary output\n",
    "        x = torch.sigmoid(self.decoder[-1](x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "vae = ConvVAE(config={'network.z_dim': 32})\n",
    "\n",
    "x = torch.randn(10, 1, 28, 28)\n",
    "print(x.shape)\n",
    "y = vae.encoder_forward(x)\n",
    "print(y.shape)\n",
    "y = y.flatten(start_dim=1)\n",
    "print(y.shape)\n",
    "# Forward pass through moment heads to obtain mu and logvar for latent variable\n",
    "mu = vae.mu_head(y)\n",
    "logvar = vae.logvar_head(y)\n",
    "print(mu.shape, logvar.shape)\n",
    "# Sample latent variable by using reparameterization trick\n",
    "z = vae.reparameterize(mu, logvar)\n",
    "print(z.shape)\n",
    "# Forward pass through decoder of sampled latent variable to obtain reconstructed input\n",
    "re_x = vae.decoder_forward(z)\n",
    "print(re_x.shape)\n",
    "# z = vae.reparameterize(y)\n",
    "# print(z.shape)\n",
    "# z = vae.decoder_forward(z)\n",
    "# print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 6, 30, 30])\n",
      "torch.Size([46, 32, 14, 14])\n",
      "torch.Size([46, 32, 14, 14])\n",
      "torch.Size([46, 64, 6, 6])\n",
      "torch.Size([46, 64, 6, 6])\n",
      "torch.Size([46, 128, 2, 2])\n",
      "torch.Size([46, 128, 2, 2])\n",
      "torch.Size([46, 512])\n",
      "torch.Size([46, 64])\n",
      "torch.Size([46, 512])\n",
      "torch.Size([46, 512])\n",
      "torch.Size([46, 128, 2, 2])\n",
      "torch.Size([46, 64, 6, 6])\n",
      "torch.Size([46, 64, 6, 6])\n",
      "torch.Size([46, 32, 14, 14])\n",
      "torch.Size([46, 32, 14, 14])\n",
      "torch.Size([46, 6, 30, 30])\n",
      "torch.Size([46, 6, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, 4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        last_dim = 128 * 4\n",
    "\n",
    "        self.mu = nn.Linear(last_dim, z_dim)\n",
    "self.logvar = nn.Linear(last_dim, z_dim)\n",
    "'''\n",
    "x = torch.Tensor(batch)\n",
    "print(x.shape)\n",
    "x = nn.Conv2d(6, 32, 4, stride=2)(x)\n",
    "print(x.shape)\n",
    "x = nn.ReLU(inplace=True)(x)\n",
    "print(x.shape)\n",
    "x = nn.Conv2d(32, 64, 4, stride=2)(x)\n",
    "print(x.shape)\n",
    "x = nn.ReLU(inplace=True)(x)\n",
    "print(x.shape)\n",
    "x = nn.Conv2d(64, 128, 4, stride=2)(x)\n",
    "print(x.shape)\n",
    "x = nn.ReLU(inplace=True)(x)\n",
    "print(x.shape)\n",
    "x = x.view(-1, 128 * 2 * 2)\n",
    "print(x.shape)\n",
    "x = nn.Linear(512, 64)(x)\n",
    "print(x.shape)\n",
    "x = nn.Linear(64, 512)(x)\n",
    "print(x.shape)\n",
    "x = nn.ReLU(inplace=True)(x)\n",
    "print(x.shape)\n",
    "x = x.view(-1, 128, 2, 2)\n",
    "print(x.shape)\n",
    "x = nn.ConvTranspose2d(128, 64, 4, stride=2)(x)\n",
    "print(x.shape)\n",
    "x = nn.ReLU(inplace=True)(x)\n",
    "print(x.shape)\n",
    "x = nn.ConvTranspose2d(64, 32, 4, stride=2)(x)\n",
    "print(x.shape)\n",
    "x = nn.ReLU(inplace=True)(x)\n",
    "print(x.shape)\n",
    "x = nn.ConvTranspose2d(32, 6, 4, stride=2)(x)\n",
    "print(x.shape)\n",
    "x = nn.ReLU(inplace=True)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 6, 30, 30])\n",
      "torch.Size([46, 6, 30, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0014,\n",
       "         0.0000, 0.0493, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0894, 0.0000, 0.0000, 0.0173, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0012, 0.0000, 0.0000],\n",
       "        [0.0051, 0.0317, 0.0000, 0.0000, 0.0542, 0.0214, 0.0637, 0.0000, 0.0450,\n",
       "         0.0000, 0.0000, 0.0000, 0.1060, 0.0000, 0.1060, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0045, 0.0064, 0.0068, 0.0376, 0.0000, 0.1211, 0.0000, 0.0037,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0905, 0.0066, 0.0329, 0.0773, 0.1327, 0.0512, 0.0865, 0.0225, 0.0291,\n",
       "         0.3268, 0.1661, 0.1018, 0.0715, 0.2939, 0.1133, 0.0000, 0.0000, 0.3505,\n",
       "         0.1562, 0.0000, 0.0000, 0.1325, 0.0393, 0.1075, 0.0000, 0.2292, 0.1605,\n",
       "         0.0099, 0.0511, 0.1018],\n",
       "        [0.0234, 0.0082, 0.0000, 0.0073, 0.0000, 0.0777, 0.0000, 0.0000, 0.0306,\n",
       "         0.0605, 0.0000, 0.0978, 0.0000, 0.0000, 0.0000, 0.0000, 0.0230, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1707, 0.0000, 0.0000,\n",
       "         0.0000, 0.0043, 0.0000],\n",
       "        [0.0290, 0.0000, 0.0405, 0.0000, 0.1815, 0.0782, 0.1044, 0.0154, 0.1725,\n",
       "         0.0000, 0.1899, 0.0789, 0.2353, 0.1485, 0.2104, 0.0215, 0.2419, 0.1040,\n",
       "         0.1651, 0.2304, 0.2830, 0.0000, 0.0000, 0.2080, 0.2429, 0.1052, 0.0399,\n",
       "         0.1302, 0.0102, 0.0194],\n",
       "        [0.0430, 0.0445, 0.0197, 0.0000, 0.0487, 0.0000, 0.0000, 0.0000, 0.0210,\n",
       "         0.0000, 0.0598, 0.0000, 0.0000, 0.0000, 0.3025, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1736, 0.0000, 0.0089, 0.0000, 0.0329,\n",
       "         0.0000, 0.0694, 0.0000],\n",
       "        [0.1293, 0.0000, 0.0129, 0.0875, 0.0408, 0.1031, 0.0424, 0.0400, 0.0014,\n",
       "         0.0887, 0.1586, 0.0623, 0.2356, 0.0956, 0.1012, 0.1680, 0.0897, 0.2766,\n",
       "         0.1371, 0.0050, 0.1979, 0.2018, 0.2935, 0.0000, 0.0000, 0.2271, 0.1914,\n",
       "         0.0000, 0.0000, 0.0667],\n",
       "        [0.0000, 0.1055, 0.0984, 0.0019, 0.0000, 0.0000, 0.0247, 0.0000, 0.1125,\n",
       "         0.0000, 0.0000, 0.0000, 0.1593, 0.0994, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0412, 0.0000, 0.0000, 0.0000, 0.1621, 0.0000, 0.0000,\n",
       "         0.0365, 0.0473, 0.0000],\n",
       "        [0.1271, 0.0000, 0.1305, 0.0308, 0.2565, 0.0000, 0.0769, 0.0000, 0.4475,\n",
       "         0.1461, 0.0582, 0.1430, 0.3871, 0.1126, 0.1995, 0.2207, 0.4657, 0.0000,\n",
       "         0.1663, 0.0954, 0.3776, 0.2862, 0.0000, 0.0561, 0.3741, 0.1113, 0.0640,\n",
       "         0.0301, 0.0000, 0.0293],\n",
       "        [0.0000, 0.1024, 0.0288, 0.0000, 0.0000, 0.0000, 0.2999, 0.0000, 0.0000,\n",
       "         0.0198, 0.1346, 0.0000, 0.0000, 0.0000, 0.1769, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.2079, 0.0000, 0.0000, 0.0372, 0.0366,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0966, 0.0000, 0.0726, 0.1263, 0.0383, 0.0452, 0.0000, 0.2908, 0.0000,\n",
       "         0.1134, 0.1875, 0.1224, 0.0000, 0.0251, 0.0934, 0.0914, 0.0000, 0.0590,\n",
       "         0.0068, 0.0202, 0.0246, 0.4150, 0.1986, 0.1352, 0.0497, 0.1539, 0.0759,\n",
       "         0.0749, 0.1333, 0.1147],\n",
       "        [0.0000, 0.1696, 0.0000, 0.0283, 0.0619, 0.0588, 0.0000, 0.0000, 0.1267,\n",
       "         0.0098, 0.0707, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1754, 0.0000,\n",
       "         0.0573, 0.0810, 0.0499, 0.0000, 0.0000, 0.0000, 0.2351, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0554, 0.0000, 0.1376, 0.1098, 0.2118, 0.1171, 0.0000, 0.0000, 0.3615,\n",
       "         0.0000, 0.2757, 0.4930, 0.3124, 0.2436, 0.1267, 0.5080, 0.3469, 0.3839,\n",
       "         0.1894, 0.2751, 0.3671, 0.2350, 0.0079, 0.1260, 0.0699, 0.0354, 0.0807,\n",
       "         0.1084, 0.0130, 0.2043],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1253, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0084, 0.2303, 0.0000, 0.0000, 0.0000,\n",
       "         0.3375, 0.0000, 0.0000, 0.0000, 0.1994, 0.0743, 0.0000, 0.0000, 0.1263,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0157, 0.0000, 0.0816, 0.0466, 0.1942, 0.3069, 0.2614, 0.0000,\n",
       "         0.3096, 0.2615, 0.0591, 0.0383, 0.0000, 0.4147, 0.0572, 0.0000, 0.1601,\n",
       "         0.3994, 0.1338, 0.0000, 0.2967, 0.3841, 0.1678, 0.0410, 0.3066, 0.2306,\n",
       "         0.2025, 0.0878, 0.2067],\n",
       "        [0.0000, 0.1229, 0.0000, 0.0008, 0.0754, 0.1636, 0.0000, 0.0000, 0.0445,\n",
       "         0.1068, 0.0000, 0.0971, 0.0000, 0.3275, 0.0000, 0.0000, 0.1368, 0.0000,\n",
       "         0.0000, 0.0000, 0.0471, 0.0028, 0.0000, 0.1563, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0093, 0.0000],\n",
       "        [0.0247, 0.0000, 0.0000, 0.0000, 0.2674, 0.0312, 0.0944, 0.0834, 0.2505,\n",
       "         0.0074, 0.1307, 0.0000, 0.3727, 0.2601, 0.0000, 0.0000, 0.3548, 0.3661,\n",
       "         0.2738, 0.3527, 0.7354, 0.0000, 0.0000, 0.1325, 0.1068, 0.2258, 0.0149,\n",
       "         0.1145, 0.1296, 0.0000],\n",
       "        [0.0097, 0.0391, 0.1038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0755, 0.0000,\n",
       "         0.0000, 0.0493, 0.0000, 0.0000, 0.0000, 0.2223, 0.0000, 0.0000, 0.1176,\n",
       "         0.0000, 0.0000, 0.0000, 0.0008, 0.1104, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0764, 0.0000, 0.0298],\n",
       "        [0.0523, 0.0000, 0.0000, 0.1494, 0.0000, 0.0163, 0.0416, 0.0000, 0.0000,\n",
       "         0.0999, 0.0000, 0.0833, 0.0000, 0.0458, 0.1404, 0.1076, 0.0000, 0.0000,\n",
       "         0.1438, 0.0797, 0.0000, 0.1996, 0.2623, 0.0409, 0.0000, 0.2077, 0.1679,\n",
       "         0.0727, 0.0000, 0.1555],\n",
       "        [0.1099, 0.0860, 0.0542, 0.0000, 0.0045, 0.0000, 0.0000, 0.0438, 0.1264,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0013, 0.0000, 0.0000, 0.1941, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1166, 0.1956, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0568, 0.0000, 0.1294, 0.0818, 0.1370, 0.0836, 0.0000, 0.1384, 0.3326,\n",
       "         0.2466, 0.3053, 0.3925, 0.4043, 0.0066, 0.0667, 0.1790, 0.4407, 0.1301,\n",
       "         0.3071, 0.2008, 0.0806, 0.0000, 0.0000, 0.2620, 0.3351, 0.1632, 0.0966,\n",
       "         0.0909, 0.0149, 0.1135],\n",
       "        [0.0000, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000, 0.0213, 0.0017, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0211, 0.1409, 0.0000, 0.0000, 0.0000,\n",
       "         0.0876, 0.0000, 0.0000, 0.0695, 0.1467, 0.0000, 0.0000, 0.0000, 0.0031,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0434, 0.0579, 0.1306, 0.0062, 0.0622, 0.0910, 0.0972, 0.0190,\n",
       "         0.0000, 0.0686, 0.1856, 0.0000, 0.1755, 0.0847, 0.1881, 0.0000, 0.2840,\n",
       "         0.4578, 0.0036, 0.1699, 0.1417, 0.3880, 0.1840, 0.0000, 0.1353, 0.1421,\n",
       "         0.0000, 0.0839, 0.1106],\n",
       "        [0.0020, 0.0662, 0.0000, 0.0182, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392,\n",
       "         0.1679, 0.0000, 0.0000, 0.0072, 0.0148, 0.0000, 0.0468, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1544, 0.0000, 0.0000, 0.0000,\n",
       "         0.0454, 0.0325, 0.0000],\n",
       "        [0.0166, 0.1052, 0.2382, 0.0197, 0.0432, 0.1949, 0.0365, 0.1265, 0.3055,\n",
       "         0.1889, 0.1329, 0.1407, 0.1007, 0.0667, 0.1795, 0.1397, 0.1750, 0.0000,\n",
       "         0.0992, 0.2137, 0.2711, 0.0000, 0.1742, 0.1784, 0.2482, 0.1392, 0.0000,\n",
       "         0.0891, 0.0987, 0.0117],\n",
       "        [0.0000, 0.0000, 0.0598, 0.0000, 0.0238, 0.0000, 0.0000, 0.0658, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0823, 0.0000, 0.0000,\n",
       "         0.0719, 0.0364, 0.0000, 0.0000, 0.0000, 0.0863, 0.0000, 0.0124, 0.0505,\n",
       "         0.0254, 0.0000, 0.0087],\n",
       "        [0.0000, 0.0000, 0.0529, 0.1719, 0.0000, 0.0250, 0.0664, 0.0000, 0.0085,\n",
       "         0.1631, 0.0809, 0.0198, 0.1515, 0.1059, 0.0977, 0.0999, 0.0000, 0.0733,\n",
       "         0.0812, 0.2299, 0.0000, 0.0000, 0.0697, 0.0864, 0.0061, 0.0594, 0.1358,\n",
       "         0.2914, 0.0366, 0.1287],\n",
       "        [0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0072, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0162, 0.0000, 0.0000, 0.0000, 0.0000, 0.0282,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0246, 0.0605, 0.0434, 0.0173, 0.0445, 0.0791, 0.0871, 0.0962, 0.1036,\n",
       "         0.1499, 0.1186, 0.1055, 0.0427, 0.0759, 0.1597, 0.0727, 0.1646, 0.0614,\n",
       "         0.0000, 0.2300, 0.1554, 0.1634, 0.1542, 0.1000, 0.0836, 0.1496, 0.0000,\n",
       "         0.1124, 0.0706, 0.1060],\n",
       "        [0.0000, 0.0149, 0.0000, 0.0000, 0.0000, 0.0166, 0.0502, 0.0029, 0.0000,\n",
       "         0.0000, 0.0079, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0003,\n",
       "         0.0000, 0.0000, 0.0087, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sc2ConvVAE(BaseVAE):\n",
    "    def make_encoder(self, config):\n",
    "        out = make_cnn(input_channel=6, \n",
    "                       channels=[32, 64, 128], \n",
    "                       kernels=[4, 4, 4], \n",
    "                       strides=[2, 2, 2], \n",
    "                       paddings=[0, 0, 0])\n",
    "        last_dim = 128 * 2 * 2\n",
    "        \n",
    "        return out, last_dim\n",
    "        \n",
    "    def make_moment_heads(self, config, last_dim):\n",
    "        out = {}\n",
    "        \n",
    "        z_dim = config['network.z_dim']\n",
    "        \n",
    "        out['mu_head'] = nn.Linear(in_features=last_dim, out_features=z_dim)\n",
    "        out['logvar_head'] = nn.Linear(in_features=last_dim, out_features=z_dim)\n",
    "        out['z_dim'] = z_dim\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def make_decoder(self, config, z_dim):\n",
    "        out = nn.ModuleList()\n",
    "        \n",
    "        out.append(nn.Linear(in_features=z_dim, out_features=self.last_dim))\n",
    "        \n",
    "        out.extend(make_transposed_cnn(input_channel=128, \n",
    "                                       channels=[64, 32, 6], \n",
    "                                       kernels=[4, 4, 4], \n",
    "                                       strides=[2, 2, 2], \n",
    "                                       paddings=[0, 0, 0], \n",
    "                                       output_paddings=[0, 0, 0]))\n",
    "        return out\n",
    "\n",
    "    def init_params(self, config):\n",
    "        for layer in self.encoder:\n",
    "            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n",
    "            \n",
    "        ortho_init(self.mu_head, nonlinearity=None, weight_scale=0.01, constant_bias=0.0)\n",
    "        ortho_init(self.logvar_head, nonlinearity=None, weight_scale=0.01, constant_bias=0.0)\n",
    "        \n",
    "        for layer in self.decoder:\n",
    "            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n",
    "\n",
    "    def encoder_forward(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder_forward(self, z):\n",
    "        # Forward of first fully-connected layer\n",
    "        x = F.relu(self.decoder[0](z))\n",
    "        \n",
    "        # Reshape as [NxCxHxW]\n",
    "        x = x.view(-1, 128, 2, 2)\n",
    "        \n",
    "        # Forward pass through transposed convolutional layer\n",
    "        for layer in self.decoder[1:]:\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "#         # Flatten to [N, D]\n",
    "#         x = x.flatten(start_dim=1)\n",
    "\n",
    "#         # Element-wise binary output\n",
    "#         x = torch.sigmoid(self.decoder[-1](x))\n",
    "\n",
    "        return x\n",
    "\n",
    "vae = Sc2ConvVAE(config={'network.z_dim': 64})\n",
    "x = torch.Tensor(batch)\n",
    "print(x.shape)\n",
    "re_x, _, _ = vae.forward(x)\n",
    "print(re_x.shape)\n",
    "re_x[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 49.87052917480469\n",
      "Reconstruction Loss: 49.870521545410156\n",
      "Kl Loss: 8.255243301391602e-06\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 27.063880920410156\n",
      "Reconstruction Loss: 26.904666900634766\n",
      "Kl Loss: 0.15921440720558167\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 27.50695037841797\n",
      "Reconstruction Loss: 27.50146484375\n",
      "Kl Loss: 0.005484968423843384\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 26.453794479370117\n",
      "Reconstruction Loss: 26.452856063842773\n",
      "Kl Loss: 0.0009378194808959961\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 67.16848754882812\n",
      "Reconstruction Loss: 67.11270141601562\n",
      "Kl Loss: 0.0557885468006134\n",
      "--------------------------------------------------\n",
      "EPOCH 1\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 31.12205696105957\n",
      "Reconstruction Loss: 30.94449806213379\n",
      "Kl Loss: 0.17755833268165588\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 31.540075302124023\n",
      "Reconstruction Loss: 29.98152732849121\n",
      "Kl Loss: 1.5585479736328125\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 10.560773849487305\n",
      "Reconstruction Loss: 3.3227553367614746\n",
      "Kl Loss: 7.238018989562988\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 33.09128189086914\n",
      "Reconstruction Loss: 28.05000114440918\n",
      "Kl Loss: 5.041281223297119\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 5.409150123596191\n",
      "Reconstruction Loss: 3.3050477504730225\n",
      "Kl Loss: 2.104102373123169\n",
      "--------------------------------------------------\n",
      "EPOCH 2\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 4.101774215698242\n",
      "Reconstruction Loss: 1.4203542470932007\n",
      "Kl Loss: 2.681420087814331\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 16.776147842407227\n",
      "Reconstruction Loss: 15.75545883178711\n",
      "Kl Loss: 1.0206882953643799\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 2.9940218925476074\n",
      "Reconstruction Loss: 1.0163427591323853\n",
      "Kl Loss: 1.9776792526245117\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 35.90460205078125\n",
      "Reconstruction Loss: 32.771785736083984\n",
      "Kl Loss: 3.132817506790161\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 30.431800842285156\n",
      "Reconstruction Loss: 26.494050979614258\n",
      "Kl Loss: 3.9377503395080566\n",
      "--------------------------------------------------\n",
      "EPOCH 3\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 11.261687278747559\n",
      "Reconstruction Loss: 9.216182708740234\n",
      "Kl Loss: 2.045504331588745\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 30.491775512695312\n",
      "Reconstruction Loss: 28.741342544555664\n",
      "Kl Loss: 1.7504323720932007\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 5.842058181762695\n",
      "Reconstruction Loss: 4.278721809387207\n",
      "Kl Loss: 1.5633363723754883\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 3.927548885345459\n",
      "Reconstruction Loss: 0.8458553552627563\n",
      "Kl Loss: 3.081693649291992\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 32.23771667480469\n",
      "Reconstruction Loss: 28.635387420654297\n",
      "Kl Loss: 3.6023287773132324\n",
      "--------------------------------------------------\n",
      "EPOCH 4\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 37.65113067626953\n",
      "Reconstruction Loss: 34.764400482177734\n",
      "Kl Loss: 2.8867313861846924\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 9.472583770751953\n",
      "Reconstruction Loss: 6.213408946990967\n",
      "Kl Loss: 3.2591748237609863\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 6.640815258026123\n",
      "Reconstruction Loss: 2.6458957195281982\n",
      "Kl Loss: 3.994919538497925\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 3.1717019081115723\n",
      "Reconstruction Loss: 0.8672429323196411\n",
      "Kl Loss: 2.3044590950012207\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 24.463300704956055\n",
      "Reconstruction Loss: 23.697301864624023\n",
      "Kl Loss: 0.7659986019134521\n",
      "--------------------------------------------------\n",
      "EPOCH 5\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 8.384334564208984\n",
      "Reconstruction Loss: 5.0772576332092285\n",
      "Kl Loss: 3.3070764541625977\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 7.648179054260254\n",
      "Reconstruction Loss: 2.5974652767181396\n",
      "Kl Loss: 5.050713539123535\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 12.18952751159668\n",
      "Reconstruction Loss: 9.152050018310547\n",
      "Kl Loss: 3.037477493286133\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 42.138336181640625\n",
      "Reconstruction Loss: 40.26080322265625\n",
      "Kl Loss: 1.877532958984375\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 4.056798934936523\n",
      "Reconstruction Loss: 0.9774486422538757\n",
      "Kl Loss: 3.079350471496582\n",
      "--------------------------------------------------\n",
      "EPOCH 6\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 5.514814853668213\n",
      "Reconstruction Loss: 1.8608157634735107\n",
      "Kl Loss: 3.653999090194702\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 60.52387237548828\n",
      "Reconstruction Loss: 57.767215728759766\n",
      "Kl Loss: 2.7566561698913574\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 31.060283660888672\n",
      "Reconstruction Loss: 28.988344192504883\n",
      "Kl Loss: 2.0719399452209473\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 22.6264705657959\n",
      "Reconstruction Loss: 20.937328338623047\n",
      "Kl Loss: 1.6891416311264038\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 8.51716423034668\n",
      "Reconstruction Loss: 4.002392292022705\n",
      "Kl Loss: 4.514772415161133\n",
      "--------------------------------------------------\n",
      "EPOCH 7\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 6.07844352722168\n",
      "Reconstruction Loss: 2.72348690032959\n",
      "Kl Loss: 3.3549563884735107\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 30.794836044311523\n",
      "Reconstruction Loss: 29.155088424682617\n",
      "Kl Loss: 1.6397478580474854\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 7.186041831970215\n",
      "Reconstruction Loss: 5.422373294830322\n",
      "Kl Loss: 1.7636687755584717\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 3.2909932136535645\n",
      "Reconstruction Loss: 1.6389216184616089\n",
      "Kl Loss: 1.6520717144012451\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 5.632036209106445\n",
      "Reconstruction Loss: 2.658310890197754\n",
      "Kl Loss: 2.9737253189086914\n",
      "--------------------------------------------------\n",
      "EPOCH 8\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 5.506521701812744\n",
      "Reconstruction Loss: 3.502959728240967\n",
      "Kl Loss: 2.0035619735717773\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 3.8887088298797607\n",
      "Reconstruction Loss: 1.9025189876556396\n",
      "Kl Loss: 1.986189842224121\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 8.633472442626953\n",
      "Reconstruction Loss: 6.891654968261719\n",
      "Kl Loss: 1.7418177127838135\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 3.4793293476104736\n",
      "Reconstruction Loss: 1.4441964626312256\n",
      "Kl Loss: 2.035132884979248\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 5.383312702178955\n",
      "Reconstruction Loss: 3.2429263591766357\n",
      "Kl Loss: 2.1403863430023193\n",
      "--------------------------------------------------\n",
      "EPOCH 9\n",
      "--------------------------------------------------\n",
      "Iteration: 0\n",
      "Train Loss: 10.085808753967285\n",
      "Reconstruction Loss: 8.575077056884766\n",
      "Kl Loss: 1.5107314586639404\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 9\n",
      "Train Loss: 4.6876726150512695\n",
      "Reconstruction Loss: 2.8944921493530273\n",
      "Kl Loss: 1.7931804656982422\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 19\n",
      "Train Loss: 7.754600524902344\n",
      "Reconstruction Loss: 4.768698692321777\n",
      "Kl Loss: 2.9859018325805664\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 29\n",
      "Train Loss: 3.4101850986480713\n",
      "Reconstruction Loss: 0.8436926007270813\n",
      "Kl Loss: 2.5664925575256348\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Iteration: 39\n",
      "Train Loss: 8.069873809814453\n",
      "Reconstruction Loss: 5.708449840545654\n",
      "Kl Loss: 2.3614237308502197\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from lagom import Logger\n",
    "\n",
    "from lagom.engine import BaseEngine\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "model = Sc2ConvVAE(config={'network.z_dim': 32})\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()  # set to training mode\n",
    "\n",
    "# Create a logger\n",
    "train_output = Logger()\n",
    "\n",
    "# Iterate over data batches for one epoch\n",
    "for epoch in range(10):\n",
    "    print('EPOCH', epoch)\n",
    "    for i, data in enumerate(loader):\n",
    "        # Put data to device\n",
    "    #     data = data.to(self.device)\n",
    "        # Zero-out gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass of data\n",
    "        data = data / 300.\n",
    "        re_x, mu, logvar = model(data)\n",
    "        # Calculate loss\n",
    "        out = model.calculate_loss(re_x=re_x, x=data, mu=mu, logvar=logvar, loss_type='MSE')\n",
    "        loss = out['loss']\n",
    "        # Backward pass to calcualte gradients\n",
    "        loss.backward()\n",
    "        # Take a gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record train output\n",
    "    #     train_output.log('epoch', n)\n",
    "        train_output.log('iteration', i)\n",
    "        train_output.log('train_loss', out['loss'].item())  # item() saves memory\n",
    "        train_output.log('reconstruction_loss', out['re_loss'].item())\n",
    "        train_output.log('KL_loss', out['KL_loss'].item())\n",
    "\n",
    "        # Dump logging\n",
    "        if i == 0 or (i+1) % 10 == 0:\n",
    "            print('-'*50)\n",
    "            train_output.dump(keys=None, index=-1, indent=0)\n",
    "            print('-'*50)\n",
    "        \n",
    "    with torch.no_grad():  # fast, disable grad\n",
    "        z = torch.randn(16, 32)\n",
    "        re_x = model.decoder_forward(z)\n",
    "        re_x = re_x.view(16, 6, 30, 30) * 5\n",
    "        me, him = re_x.split(3, dim=1)\n",
    "        save_image(me, f'sample_me_{epoch}.png')\n",
    "        save_image(me, f'sample_him_{epoch}.png')\n",
    "        \n",
    "\n",
    "    # train_output.logs\n",
    "\n",
    "#     def log_train(self, train_output, **kwargs):\n",
    "#         logdir = kwargs['logdir']\n",
    "#         epoch = kwargs['epoch']\n",
    "        \n",
    "#         mean_loss = np.mean(train_output['train_loss'])\n",
    "#         print(f'====> Average loss: {mean_loss}')\n",
    "        \n",
    "#         # Use decoder to sample images from standard Gaussian noise\n",
    "#         with torch.no_grad():  # fast, disable grad\n",
    "#             z = torch.randn(64, self.config['network.z_dim']).to(self.device)\n",
    "#             re_x = self.agent.decoder_forward(z).cpu()\n",
    "#             save_image(re_x.view(64, 1, 28, 28), f'{logdir}/sample_{epoch}.png')\n",
    "\n",
    "#     def eval(self, n=None):\n",
    "#         self.agent.eval()  # set to evaluation mode\n",
    "        \n",
    "#         # Create a logger\n",
    "#         eval_output = Logger()\n",
    "        \n",
    "#         # Iterate over test batches\n",
    "#         for i, (data, label) in enumerate(self.test_loader):\n",
    "#             # Put data to device\n",
    "#             data = data.to(self.device)\n",
    "#             with torch.no_grad():  # fast, disable grad\n",
    "#                 # Forward pass of data\n",
    "#                 re_x, mu, logvar = self.agent(data)\n",
    "#                 # Calculate loss\n",
    "#                 out = self.agent.calculate_loss(re_x=re_x, x=data, mu=mu, logvar=logvar, loss_type='BCE')\n",
    "            \n",
    "#             # Record eval output\n",
    "#             eval_output.log('eval_loss', out['loss'].item())\n",
    "        \n",
    "#         return eval_output.logs\n",
    "\n",
    "#     def log_eval(self, eval_output, **kwargs):\n",
    "#         logdir = kwargs['logdir']\n",
    "#         epoch = kwargs['epoch']\n",
    "        \n",
    "#         mean_loss = np.mean(eval_output['eval_loss'])\n",
    "#         print(f'====> Test set loss: {mean_loss}')\n",
    "        \n",
    "#         # Reconstruct some test images\n",
    "#         data, label = next(iter(self.test_loader))  # get a random batch\n",
    "#         data = data.to(self.device)\n",
    "#         n = min(data.size(0), 8)  # number of images\n",
    "#         D = data[:n]\n",
    "#         with torch.no_grad():  # fast, disable grad\n",
    "#             re_x, _, _ = self.agent(D)\n",
    "#         compare_img = torch.cat([D.cpu(), re_x.cpu().view(-1, 1, 28, 28)])\n",
    "#         save_image(compare_img, f'{logdir}/reconstruction_{epoch}.png', nrow=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.Adam??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
